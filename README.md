# Comparison-of-Different-Upsampling-Techniques-in-Machine-Learning

This project considered the effects of different upsampling techniques in improving recall, which have great essence in the prediction of the minority class.
To achieve enough flexibility and comprehensiveness, three churn datassets with problem of imbalance were used across five upsampling techniques. These upsampling techniques include SMOTE, SVM SMOTE, ADASYN, Borderline SMOTE, and Random oversampler.

For each of the datasets again, as shown in each notebooks, the performance of six different machine learning classifiers were accessed. These classifiers are Random Forest, KNN, Logistic Regression, AdaBoost, GradientBoost, and XGBoost. The evaluation metric used in the project are Accuracy,	Precision,	Recall,	Specificity,	G-Mean,	Roc-Score, and 	MCC.

For comprehensiveness of results, the visualization ipynb file shows the results of the classifiers for the three datasets. The visualization aims to show how well the models perform across the different upsampling techniques.

